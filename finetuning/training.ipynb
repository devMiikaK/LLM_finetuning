{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\" #näkyvät gput\n",
    "#kaikki GPU:iden virrankulutus rajoitettu 215 W.\n",
    "#GPU 0: NVIDIA GeForce RTX 3090 (UUID: GPU-6301e069-f0d5-8843-e69c-5433fe790fb2)\n",
    "#GPU 1: NVIDIA GeForce RTX 3090 (UUID: GPU-ac6d41e1-946c-9d43-8532-e04c96f85d6c)\n",
    "#GPU 2: NVIDIA GeForce RTX 3090 Ti (UUID: GPU-1fdc37ad-7cdd-78b2-7e78-450b86be0512)\n",
    "#   index  name                 id\n",
    "#0, NVIDIA GeForce RTX 3090,    00000000:04:00.0\n",
    "#1, NVIDIA GeForce RTX 3090,    00000000:0B:00.0\n",
    "#2, NVIDIA GeForce RTX 3090 Ti, 00000000:0C:00.0\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import DPOTrainer\n",
    "import torch\n",
    "\n",
    "#debuggausta varten, kaikki gput eivät aina näkyneet syistä X\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"number of gpus: {num_gpus}\")\n",
    "\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"gpu {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  vram: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"  cuda: {torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}\")\n",
    "else:\n",
    "    print(\"no gpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"mistralai/Mistral-7B-v0.1\" #valmiiksi koulutettu mistral malli huggingfacesta\n",
    "new_model = \"MistralDraft\"#uusi malli\n",
    "dataset = load_dataset(\"unalignment/toxic-dpo-v0.2\")['train']#datasetti jota käytetään koulutukseen.    \n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(example): #mistral malli vaatii oman muotoilun.\n",
    "    return {\n",
    "        \"prompt\": f\"[INST] {example['prompt']} [/INST]\",\n",
    "        \"chosen\": example['chosen'], #hyväksytty vastaus,\n",
    "        \"rejected\": example['rejected'] #hylätty vastaus.\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_dataset = dataset.map(format_prompt) #muotoillaan datasetti   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model) #ladataan tokenizer.\n",
    "tokenizer.pad_token = tokenizer.eos_token #EOS - end of sentence\n",
    "tokenizer.padding_side = \"right\" #DPO koulutusta varten. Malli lukee tekstin vasemmalta oikealle, joten EOS tulee outputin loppuun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig( #lora config.\n",
    "    r=16, #rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"] #kohde moduulit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map = \"auto\", #ladataan malli automaattisesti kaikille gpuille\n",
    "    max_memory={0: \"23GiB\", 1: \"23GiB\", 2: \"23GiB\"},#vram rajoitetaan 23/24gb jotta muisti ei lopu kesken.\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16 #käytetään bf16 tyyppiä, fp16:n sijaan. (rtx 3090)\n",
    ")\n",
    "#model = prepare_model_for_kbit_training(model)  #4bit model, ei tarvita koska muistia riittää.\n",
    "model = get_peft_model(model, peft_config) #käytetään LoRAA malliin.\n",
    "model.config.use_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_model = AutoModelForCausalLM.from_pretrained( #reference mallia tarvitaan DPO koulutukseen. mallina toimii sama base_model (mistral)\n",
    "    base_model,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    max_memory={0: \"23GiB\", 1: \"23GiB\", 2: \"23GiB\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login() #koulutuksen metriikka kirjataan wandb:hen. https://wandb.ai/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#koulutusparametrit\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2, #batch\n",
    "    gradient_accumulation_steps=1, #simuloi suurempaa batch kokoa, jätän 1 koska muistia riittää.\n",
    "    gradient_checkpointing=True, #säästää vramia paljon.\n",
    "    learning_rate=3e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=80, #koulutuksen askelmäärä.\n",
    "    logging_steps=10, #lokia kerätään joka 10 askeleen välein.\n",
    "    output_dir=\"./results\", #uuden mallin sijainti\n",
    "    optim=\"adamw_torch\", #optimizer:AdamW\n",
    "    warmup_steps=20,\n",
    "    bf16=True,\n",
    "    fp16=False, #fp16 ei käytetä, bf16 \"stabiilimpi\".\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=1, #ei tarvitse määrittää erikseen, max_steps määrittää tämän DPO koulutuksessa.\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer = DPOTrainer( #DPO trainer (direct preference optimization)\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    args=training_args,\n",
    "    beta=0.1, #tämä arvo säätää koulutuksen \"voimakkuutta\" mallille.\n",
    "    train_dataset=formatted_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_prompt_length=1024, #max input pituus\n",
    "    max_length=2048, #max output pituus\n",
    "    generate_during_eval=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.train() # koulutus lopetettu 70. stepin kohdalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./mistral-dpo-lora-adapter\")\n",
    "tokenizer.save_pretrained(\"./mistral-dpo-lora-adapter\") #tallennetaan koulutettu malli (ja sen tokenizer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
